{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2. IWAL algorithm implementation (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this question is to implement Importance Weighted Active Learning (IWAL) algorithm. For this question, you will not use modAL, but instead will implement IWAL routine from scratch using scikit-learn, NumPy and native Python. \n",
    "\n",
    "In this question, we will use a simple synthetic dataset for a binary classification problem. Each data point has only 2 features. The dataset is provided in 2 files -- â€œdata_iwal.npyâ€, which contains features and â€œlabels_iwal.npyâ€, which contains labels. \n",
    "\n",
    "For simplicity, you will implement bootstrapping rejection sampling subroutine with logistic regression and hinge loss.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğ‚ğ¨ğ¦ğ©ğ¥ğğ­ğ ğ­ğ¡ğ ğœğ¨ğğ ğ®ğ§ğğğ« ###ğ“ğ ğƒğ ğ¢ğ§ ğğšğœğ¡ ğœğğ¥ğ¥ ğšğ§ğ ğ©ğ«ğ¨ğğ®ğœğ ğ­ğ¡ğ ğ«ğğªğ®ğ¢ğ«ğğ ğ©ğ¥ğ¨ğ­ğ¬.  Feel free to define any helper functions as you see fit. You may import and use any modules in scikit-learn and NumPy to help with your implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import necessary modules. Feel free to add something else here if you need it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hinge_loss, log_loss\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read the data and split it into train and test datasets. Train will be used to train our classification model and test will be used to validate the performance, monitor overfitting and compare the results of the model trained with Active Learning with the ones of the model trained from scratch. We set aside 1/3 of the dataset for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"data/q2/data_iwal.npy\")\n",
    "y = np.load(\"data/q2/labels_iwal.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 2)\n",
      "(134,)\n",
      "(66, 2)\n",
      "(66,)\n",
      "[[2.59193175 1.14706863]\n",
      " [1.7756532  1.15670278]]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(X[0:2])\n",
    "print(y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1\n",
    "Type your answers for the theoretical questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the idea behind IWAL algorithm?\n",
    "\n",
    "**Your answer goes here**\n",
    "\n",
    "2. What are the assumptions made for the IWAL algorithm?\n",
    "\n",
    "**Your answer goes here**\n",
    "\n",
    "3. What are the pros and cons of IWAL algorithm?\n",
    "\n",
    "**Your answer goes here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2 Implement IWAL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you will implement a function that performs a single query of Algorithm 1 IWAL (subroutine rejection-sampling) from the paper. Below is the function description that you can follow in your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented Equations\n",
    "\n",
    "$h_t = \\underset{h \\in H}{argmin}\\sum_{(x,y,c)\\in S_t}c \\cdot l(h(x),y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented in custom Python package iwal_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3 Implement bootstrapping rejection sampling subroutine\n",
    "In this part you will implement bootstrapping rejection sampling subroutine from the paper, section 7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented Equations\n",
    "$p_t = p_{min}+(1-p_{min})[\\underset{y;h_i,h_j \\in H}{max}L(h_i(x),y)-L(h_j(x),y)]$\n",
    "\n",
    "where $p_{min}$ is a lower bound on the sampling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented in custom Python package iwal_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.4 Organize all implemented parts into a single pipeline\n",
    "You implemented all parts of the IWAL algorithm with bootstrap rejection sampling. Now organize it into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_random(X,y,n):\n",
    "    index = np.random.choice(X.shape[0], n, replace=True)\n",
    "    return X[index],y[index]\n",
    "\n",
    "#?? isn't initializing H creating n_h classifiers?\n",
    "#?? initialize history means to simply set up the empty dictionary?\n",
    "\n",
    "#?? what do we use initial training set for? if we use on all models, all models will have the same behavior...\n",
    "#?? need to fit models first? what data do I use? \n",
    "#?? data sampled with replacement? I thought we are using the first 10 recoreds as bootstrapped?\n",
    "#?? train each model on different set of training data?\n",
    "#?? don't understand bootstrapped data here? remove the bootstrapped data from our pool/stream for future queries?\n",
    "\n",
    "#?? are we supposed to retrain the model, with new data?\n",
    "#?? how to choose stream of samples? start from beginning?\n",
    "#?? why is log_loss also imported? why use hinge_loss in algorithm but log_loss here?\n",
    "\n",
    "#     print(x_t.shape)\n",
    "#     print(y_t.shape)\n",
    "#     print(x_t)\n",
    "#     print(y_t)\n",
    "#     print(type(y_t))\n",
    "    \n",
    "#     predict = None\n",
    "#     for each in H:\n",
    "#         a = each.predict(x_t)\n",
    "        \n",
    "#         if not predict:\n",
    "#             predict = a\n",
    "#         else:\n",
    "#             if a != predict:\n",
    "#                 print('Found a difference!')\n",
    "#                 print(predict)\n",
    "#                 print(a)\n",
    "#                 print(x_t)\n",
    "#                 print(y_t)\n",
    "\n",
    "# create an initial training set\n",
    "n_initial = 10\n",
    "X_training, y_training = X_train[:n_initial], y_train[:n_initial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected model: None\n",
      "selected model: None\n",
      "selected model: None\n",
      "selected model: None\n",
      "selected model: None\n",
      "selected model: None\n",
      "selected model: None\n",
      "selected model: None\n",
      "selected model: None\n",
      "selected model: None\n",
      "[array([[1.71950845, 1.33191447]]), array([[0.25230009, 4.78090349]]), array([[1.25790923, 3.82442026]]), array([[0.11846522, 4.89484141]]), array([[1.37988365, 4.1579782 ]]), array([[2.03823923, 0.38104202]]), array([[2.29447557, 3.36035598]]), array([[2.59654461, 0.06894721]]), array([[1.38610602, 0.65547642]]), array([[1.48663347, 4.39407536]])]\n",
      "[[1.71950845 1.33191447]\n",
      " [0.25230009 4.78090349]\n",
      " [1.25790923 3.82442026]\n",
      " [0.11846522 4.89484141]\n",
      " [1.37988365 4.1579782 ]\n",
      " [2.03823923 0.38104202]\n",
      " [2.29447557 3.36035598]\n",
      " [2.59654461 0.06894721]\n",
      " [1.38610602 0.65547642]\n",
      " [1.48663347 4.39407536]]\n",
      "(10, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 20 into shape (10,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/ryanquinnnelson/CMU-02750-HW1/packages/iwal/iwal.py\u001b[0m in \u001b[0;36miwal_query\u001b[0;34m(x_t, y_t, hypothesis_space, history, selected, labels, rejection_threshold, bootstrap_size, p_min)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# calculate probability of requesting label for x_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrejection_threshold\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bootstrap'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mp_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrap_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Function does not support rejection_threshold:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejection_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/ryanquinnnelson/CMU-02750-HW1/packages/iwal/rejection_threshold.py\u001b[0m in \u001b[0;36mbootstrap\u001b[0;34m(x_t, h_space, bootstrap_size, history, labels, p_min)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# consider whether to also train predictors on the initial sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbootstrap_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# training set size is equal to the bootstrap size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0m_bootstrap_train_predictors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mmax_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bootstrap_calculate_max_loss_difference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_bootstrap_ldf_hinge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/ryanquinnnelson/CMU-02750-HW1/packages/iwal/rejection_threshold.py\u001b[0m in \u001b[0;36m_bootstrap_train_predictors\u001b[0;34m(hypothesis_space, history)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_bootstrap_train_predictors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bootstrap_reshape_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# entire history so far consists of selected samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypothesis_space\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/ryanquinnnelson/CMU-02750-HW1/packages/iwal/rejection_threshold.py\u001b[0m in \u001b[0;36m_bootstrap_reshape_history\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mX_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mX_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# reshape y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 20 into shape (10,1)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from packages.iwal import iwal  # my custom package\n",
    "\n",
    "# fill hypothesis_space\n",
    "H = []\n",
    "num_h = 10\n",
    "for i in range(num_h):\n",
    "    lr = LogisticRegression()\n",
    "    H.append(lr)\n",
    "\n",
    "# additional arguments\n",
    "bootstrap_size = 10\n",
    "labels = [0,1]\n",
    "p_min = 0.1  \n",
    "history = {'X': [],'y': [],'c': [],'Q': []}\n",
    "selected = []\n",
    "rejection_threshold = 'bootstrap'\n",
    "\n",
    "# Perform queries and record loss\n",
    "losses = []\n",
    "n_query = 1\n",
    "for t in range(n_query):\n",
    "    \n",
    "    # select sample \n",
    "    x_t = X_train[t].reshape(1,2)\n",
    "    y_t = y_train[t].reshape(1,)\n",
    "    \n",
    "    # select optimal hypothesis\n",
    "    h_t = iwal.iwal_query(x_t,y_t,H,history,selected,labels,rejection_threshold,bootstrap_size,p_min)\n",
    "\n",
    "    print('selected model:',h_t)\n",
    "\n",
    "#     # calculate loss once models have been fitted\n",
    "#     if t > bootstrap_size:\n",
    "#         # calculate loss and store for later\n",
    "#         loss_t = log_loss(y_test, h_t.predict_proba(X_test)) \n",
    "#         losses.append(loss_t)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[1.71950845 1.33191447]] y: [1] c: 1.0 Q: 1\n",
      "X: [[0.25230009 4.78090349]] y: [0] c: 1.0 Q: 1\n",
      "X: [[1.25790923 3.82442026]] y: [0] c: 1.0 Q: 1\n",
      "X: [[0.11846522 4.89484141]] y: [0] c: 1.0 Q: 1\n",
      "X: [[1.37988365 4.1579782 ]] y: [0] c: 1.0 Q: 1\n",
      "X: [[2.03823923 0.38104202]] y: [1] c: 1.0 Q: 1\n",
      "X: [[2.29447557 3.36035598]] y: [0] c: 1.0 Q: 1\n",
      "X: [[2.59654461 0.06894721]] y: [1] c: 1.0 Q: 1\n",
      "X: [[1.38610602 0.65547642]] y: [1] c: 1.0 Q: 1\n",
      "X: [[1.48663347 4.39407536]] y: [0] c: 1.0 Q: 1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6f26919c895b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'c:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Q:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for t in range(n_query):\n",
    "    print('X:', history['X'][t],'y:',history['y'][t],'c:',history['c'][t],'Q:',history['Q'][t],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.5 Compare results of Active Learning vs No Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you need to create object of the same scikit learning class and train it on randomly selected subset of data points and compare results of 2 classifiers. Comment on your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to no Active Learning setting\n",
    "# ?? how much data to train it on? same as A.L.?\n",
    "\n",
    "## TODO: your code goes here\n",
    "# select random training set\n",
    "X_rand, y_rand = select_n_random(X_train, y_train, 50)\n",
    "\n",
    "# train model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_rand, y_rand)\n",
    "\n",
    "offline_loss = log_loss(y_test, lr.predict_proba(X_test)) \n",
    "\n",
    "print('Active Learning loss:', losses[-1])\n",
    "print('Offline loss (no active learning):',offline_loss)b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on observations\n",
    "It appears that offline loss is less than active learning loss for a small "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
